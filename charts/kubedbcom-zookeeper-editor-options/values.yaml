# Default values for kubedbcom-zookeeper-editor-options.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

metadata:
  resource:
    group: kubedb.com
    kind: ZooKeeper
    name: zookeepers
    scope: Namespaced
    version: v1alpha2
  release:
    # Release name
    name: ""
    # Release namespace
    namespace: ""

spec:
  # Annotations to add to the database custom resource
  annotations: {}

  # Labels to add to all the template objects
  labels: {}

  # Standalone, Replicaset
  mode: Replicaset

  replicas: 3
  deletionPolicy: WipeOut

  persistence:
    size: 10Gi

  podResources:
    machine: ""
    resources:
      limits:
        cpu: 500m
        memory: 1Gi

  authSecret:
    name: ""
    password: ""

  admin:
    deployment:
      default: Dedicated
      toggle: true
    clusterTier:
      default: "GeneralPurpose"
      toggle: true
      nodeTopology:
        available: ["m4", "m7gd", "c5d", "c7g", "t4g", "r5a", "r6g", "a-family", "b-family", "d-family", "standard-bsv2-family", "standard-ddv4-family", "standard-dv2-family"]
        default: "standard-bsv2-family"
        toggle: true
      placement:
        available: ["majority-on-spot", "minority-on-spot", "multizone", "ondemand-only", "one-on-ondemand", "spot-only", "zone-ap-southeast-1a", "zone-ap-southeast-1b", "zone-ap-southeast-1c", "default"]
        default: "default"
        toggle: true

    databases:
      ZooKeeper:
        versions:
          available: ["3.8.3", "3.9.1"]
          default: "3.8.3"
          toggle: true
    storageClasses:
      available: ["linode-block-storage", "default", "standard", "gp2"]
      default: "default"
      toggle: true

    tls:
      default: false
      toggle: false
    clusterIssuers:
      available: ["global-ca", "cluster-issuer"]
      default: "cluster-issuer"
      toggle: true
    webUI:
      default: false
      toggle: false

    monitoring:
      # Name of monitoring agent (one of "prometheus.io", "prometheus.io/operator", "prometheus.io/builtin")
      agent: prometheus.io/operator
      exporter:
        resources: # +doc-gen:break
          requests:
            cpu: 100m
            memory: 128Mi
      serviceMonitor:
        # Specify the labels for ServiceMonitor.
        # Prometheus crd will select ServiceMonitor using these labels.
        # Only usable when monitoring agent is `prometheus.io/webhook server`.
        labels:
          monitoring.appscode.com/prometheus: federated
      toggle: true
    alerts:
      toggle: false

    archiver:
      toggle: false
      default: false
    backup:
      tool: ""
      toggle: false

      kubestash:
        schedule: "0 */2 * * *"
        storageRef:
          name: default
          namespace: stash
        retentionPolicy:
          name: "keep-1mo"
          namespace: stash
        encryptionSecret:
          name: default-encryption-secret
          namespace: stash

      stash:
        schedule: "0 */2 * * *"

        retentionPolicy:
          name: keep-last-30d
          keepHourly: 24
          keepDaily: 30
          prune: true

        authSecret:
          name: ""
          password: ""

        backend:
          provider: "" # s3,gcs,azure,swift,b2
          s3:
            spec:
              endpoint: ""
              bucket: ""
              # prefix: ""
              # region: ""
            auth:
              AWS_ACCESS_KEY_ID: ""
              AWS_SECRET_ACCESS_KEY: ""
              CA_CERT_DATA: ""
          azure:
            spec:
              container: ""
              # prefix: ""
              # maxConnections: 0
            auth:
              AZURE_ACCOUNT_NAME: ""
              AZURE_ACCOUNT_KEY: ""
          gcs:
            spec:
              bucket: ""
              # prefix: ""
              # maxConnections: 0
            auth:
              GOOGLE_PROJECT_ID: ""
              GOOGLE_SERVICE_ACCOUNT_JSON_KEY: ""
          swift:
            spec:
              container: ""
              # prefix: ""
            auth:
              OS_USERNAME: ""
              OS_PASSWORD: ""
              OS_REGION_NAME: ""
              OS_AUTH_URL: ""
              OS_USER_DOMAIN_NAME: ""
              OS_PROJECT_NAME: ""
              OS_PROJECT_DOMAIN_NAME: ""
              OS_TENANT_ID: ""
              OS_TENANT_NAME: ""
              ST_AUTH: ""
              ST_USER: ""
              ST_KEY: ""
              OS_STORAGE_URL: ""
              OS_AUTH_TOKEN: ""
          b2:
            spec:
              bucket: ""
              # prefix: ""
              # maxConnections: 0
            auth:
              B2_ACCOUNT_ID: ""
              B2_ACCOUNT_KEY: ""

form:
  alert:
    ## Enable PrometheusRule alerts
    enabled: warning

    ## Labels for default rules
    labels: # +doc-gen:break
      release: kube-prometheus-stack

    ## Annotations for default rules
    annotations: {}

    ## Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}

    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    # runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

    groups:
      database:
        enabled: warning
        rules:
          # ZooKeeper is down
          zookeeperDown:
            enabled: true
            duration: "1m"
            severity: critical
          # ZooKeeper has way too nodes. Consider scaling up.
          zookeeperTooManyNodes:
            enabled: true
            duration: "1m"
            val: 1000000
            severity: warning
          # ZooKeeper is allocated too big memory
          zookeeperTooBigMemory:
            enabled: true
            duration: "1m"
            val: 1
            severity: warning
          # ZooKeeper is watching many nodes
          zookeeperTooManyWatch:
            enabled: true
            duration: "1m"
            val: 10000
            severity: warning
          # ZooKeeper has created too many connections
          zookeeperTooManyConnections:
            enabled: true
            duration: "1m"
            val: 60
            severity: warning
          # ZooKeeper leader election happened
          zookeeperLeaderElection:
            enabled: true
            duration: "1m"
            severity: warning
          # Openfile count in zookeeper is more than 300
          zookeeperTooManyOpenFiles:
            enabled: true
            duration: "1m"
            val: 300
            severity: warning
          # ZooKeeper is taking more time to sync
          zookeeperTooLongFsyncTime:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # ZooKeeper is taking more time when taking snapshots
          zookeeperTooLongSnapshotTime:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # Average latency is too high
          zookeeperTooHighAvgLatency:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # ZooKeeper jvm memory is filling up
          zookeeperJvmMemoryFilingUp:
            enabled: true
            duration: "1m"
            val: 0.8
            severity: warning
          # Sepcifies the alert configuration for persistent volume usages.
          diskUsageHigh:
            enabled: true
            val: 80
            duration: "1m"
            severity: warning
          diskAlmostFull:
            enabled: true
            val: 95
            duration: "1m"
            severity: critical
      provisioner:
        enabled: warning
        rules:
          appPhaseNotReady:
            enabled: true
            duration: "1m"
            severity: critical
          appPhaseCritical:
            enabled: true
            duration: "15m"
            severity: warning
