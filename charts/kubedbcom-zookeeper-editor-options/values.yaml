# Default values for kubedbcom-zookeeper-editor-options.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

metadata:
  resource:
    group: kubedb.com
    kind: ZooKeeper
    name: zooKeepers
    scope: Namespaced
    version: v1alpha2
  release:
    # Release name
    name: ""
    # Release namespace
    namespace: ""

spec:
  # List options
  version: 3.8.3

  # Annotations to add to the database custom resource
  annotations: {}

  # Labels to add to all the template objects
  labels: {}

  # Standalone, Cluster
  mode: Standalone
  cluster:
    name: rs0
    replicas: 3

  deletionPolicy: WipeOut

  storageClass:
    name: standard

  persistence:
    size: 10Gi

  podResources:
    machine: ""
    resources:
      limits:
        cpu: 500m
        memory: 1Gi

  authSecret:
    name: ""
    password: ""

  monitoring:
    # Name of monitoring agent (one of "prometheus.io", "prometheus.io/operator", "prometheus.io/builtin")
    agent: prometheus.io/operator
    exporter:
      resources: # +doc-gen:break
        requests:
          cpu: 100m
          memory: 128Mi
    serviceMonitor:
      # Specify the labels for ServiceMonitor.
      # Prometheus crd will select ServiceMonitor using these labels.
      # Only usable when monitoring agent is `prometheus.io/webhook server`.
      labels: {}

  backup:
    tool: ""

    kubestash:
      schedule: "0 */2 * * *"
      storageRef:
        name: default
        namespace: ""
      retentionPolicy:
        name: "keep-1mo"
        namespace: ""
      encryptionSecret:
        name: default-encryption-secret
        namespace: ""
      storageSecret:
        create: true

    stash:
      schedule: "0 */2 * * *"

      retentionPolicy:
        name: keep-last-30d
        keepHourly: 24
        keepDaily: 30
        prune: true

      authSecret:
        name: ""
        password: ""

      backend:
        provider: "" # s3,gcs,azure,swift,b2
        s3:
          spec:
            endpoint: ""
            bucket: ""
            # prefix: ""
            # region: ""
          auth:
            AWS_ACCESS_KEY_ID: ""
            AWS_SECRET_ACCESS_KEY: ""
            CA_CERT_DATA: ""
        azure:
          spec:
            container: ""
            # prefix: ""
            # maxConnections: 0
          auth:
            AZURE_ACCOUNT_NAME: ""
            AZURE_ACCOUNT_KEY: ""
        gcs:
          spec:
            bucket: ""
            # prefix: ""
            # maxConnections: 0
          auth:
            GOOGLE_PROJECT_ID: ""
            GOOGLE_SERVICE_ACCOUNT_JSON_KEY: ""
        swift:
          spec:
            container: ""
            # prefix: ""
          auth:
            OS_USERNAME: ""
            OS_PASSWORD: ""
            OS_REGION_NAME: ""
            OS_AUTH_URL: ""
            OS_USER_DOMAIN_NAME: ""
            OS_PROJECT_NAME: ""
            OS_PROJECT_DOMAIN_NAME: ""
            OS_TENANT_ID: ""
            OS_TENANT_NAME: ""
            ST_AUTH: ""
            ST_USER: ""
            ST_KEY: ""
            OS_STORAGE_URL: ""
            OS_AUTH_TOKEN: ""
        b2:
          spec:
            bucket: ""
            # prefix: ""
            # maxConnections: 0
          auth:
            B2_ACCOUNT_ID: ""
            B2_ACCOUNT_KEY: ""

form:
  alert:
    ## Enable PrometheusRule alerts
    enabled: warning

    ## Labels for default rules
    labels: # +doc-gen:break
      release: kube-prometheus-stack

    ## Annotations for default rules
    annotations: {}

    ## Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}

    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    # runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

    groups:
      database:
        enabled: warning
        rules:
          # ZooKeeper is down
          zookeeperDown:
            enabled: true
            duration: "1m"
            severity: critical
          # ZooKeeper has way too nodes. Consider scaling up.
          zookeeperTooManyNodes:
            enabled: true
            duration: "1m"
            val: 1000000
            severity: warning
          # ZooKeeper is allocated too big memory
          zookeeperTooBigMemory:
            enabled: true
            duration: "1m"
            val: 1
            severity: warning
          # ZooKeeper is watching many nodes
          zookeeperTooManyWatch:
            enabled: true
            duration: "1m"
            val: 10000
            severity: warning
          # ZooKeeper has created too many connections
          zookeeperTooManyConnections:
            enabled: true
            duration: "1m"
            val: 60
            severity: warning
          # ZooKeeper leader election happened
          zookeeperLeaderElection:
            enabled: true
            duration: "1m"
            severity: warning
          # Openfile count in zookeeper is more than 300
          zookeeperTooManyOpenFiles:
            enabled: true
            duration: "1m"
            val: 300
            severity: warning
          # ZooKeeper is taking more time to sync
          zookeeperTooLongFsyncTime:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # ZooKeeper is taking more time when taking snapshots
          zookeeperTooLongSnapshotTime:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # Average latency is too high
          zookeeperTooHighAvgLatency:
            enabled: true
            duration: "1m"
            val: 100
            severity: warning
          # ZooKeeper jvm memory is filling up
          zookeeperJvmMemoryFilingUp:
            enabled: true
            duration: "1m"
            val: 0.8
            severity: warning
          # Sepcifies the alert configuration for persistent volume usages.
          diskUsageHigh:
            enabled: true
            val: 80
            duration: "1m"
            severity: warning
          diskAlmostFull:
            enabled: true
            val: 95
            duration: "1m"
            severity: critical
      provisioner:
        enabled: warning
        rules:
          appPhaseNotReady:
            enabled: true
            duration: "1m"
            severity: critical
          appPhaseCritical:
            enabled: true
            duration: "15m"
            severity: warning
  capi:
    provider: ""
    namespace: ""
    clusterName: ""
    dedicated: false
    nodes: 1
    sku: ""
    zones: []
